{"cells":[{"cell_type":"markdown","metadata":{"id":"tihwe3gZv25U"},"source":["Models Used\n","ResNet-50,\n","VGG-16,\n","EfficientNet-B0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dgdihxTv7jS","outputId":"cb04b90e-0d96-42e1-e35a-5479bd019f13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files in /content: ['.config', 'VOC2008.tar', 'VOC2008', '.ipynb_checkpoints', 'sample_data']\n","Extracting dataset...\n","Extraction complete!\n"]}],"source":["import os\n","import zipfile\n","import torch\n","import torchvision.transforms as transforms\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import DataLoader\n","import tarfile\n","\n","print(\"Files in /content:\", os.listdir(\"/content\"))\n","tar_path = \"/content/VOC2008.tar\"\n","extract_path = \"/content/VOC2008\"\n","\n","if os.path.exists(tar_path):\n","    print(\"Extracting dataset...\")\n","    with tarfile.open(tar_path, \"r\") as tar:\n","        tar.extractall(path=extract_path)\n","    print(\"Extraction complete!\")\n","else:\n","    print(\"ERROR: .tar file not found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-RtKG4qrIdY","outputId":"49cd4fc7-ed55-41ed-af36-e49ee57bf42b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset loaded successfully!\n"]}],"source":["import torchvision.transforms as transforms\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import DataLoader\n","\n","\n","data_root = \"/content/VOC2008\"\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","train_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"train\", transform=transform)\n","val_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"val\", transform=transform)\n","\n","\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","print(\"Dataset loaded successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w-sctcDrOEO","outputId":"f00dfa2e-dc72-4065-aa2b-dc50d4f7c24e"},"outputs":[{"name":"stdout","output_type":"stream","text":["DataLoaders saved!\n"]}],"source":["import torch\n","\n","torch.save(train_loader, \"/content/train_loader.pth\")\n","torch.save(val_loader, \"/content/val_loader.pth\")\n","\n","print(\"DataLoaders saved!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuSvl5L_sA5n","outputId":"ab13f519-2a0c-4a06-f59c-5699f98f857e"},"outputs":[{"name":"stdout","output_type":"stream","text":["DataLoaders successfully reloaded!\n"]}],"source":["import torchvision.transforms as transforms\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import DataLoader\n","\n","\n","data_root = \"/content/VOC2008\"\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","train_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"train\", transform=transform)\n","val_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"val\", transform=transform)\n","\n","\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","print(\"DataLoaders successfully reloaded!\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVnordA4wg3c","outputId":"e9c527e7-e4af-4e50-c7e4-a7a73b5a52a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… DataLoader is set up correctly!\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import VOCDetection\n","import torchvision.transforms as transforms\n","\n","\n","data_root = \"/content/VOC2008\"\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","VOC_CLASSES = [\n","    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n","    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n","]\n","class_to_idx = {cls: idx for idx, cls in enumerate(VOC_CLASSES)}\n","\n","\n","def voc_collate_fn(batch):\n","    images, labels = [], []\n","\n","    for img, target in batch:\n","        images.append(img)\n","\n","\n","        objects = target['annotation'].get('object', [])\n","        if isinstance(objects, dict):\n","            objects = [objects]\n","\n","\n","        label_vector = torch.zeros(len(VOC_CLASSES), dtype=torch.float32)\n","        for obj in objects:\n","            if obj['name'] in class_to_idx:\n","                label_vector[class_to_idx[obj['name']]] = 1.0\n","\n","        labels.append(label_vector)\n","\n","    images = torch.stack(images, dim=0)\n","    labels = torch.stack(labels, dim=0)\n","\n","    return images, labels\n","\n","\n","train_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"train\", transform=transform)\n","val_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"val\", transform=transform)\n","\n","\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=voc_collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=voc_collate_fn)\n","\n","print(\"âœ… DataLoader is set up correctly!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kLtKJF9wg2k"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n","    model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n","\n","    print(\"âœ… Training complete!\")\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASZWUwthhrIC","outputId":"d36ac019-0cbd-44cb-d4ad-0f503009217f"},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… DataLoader Ready!\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-29-db56eae1d350>:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n","<ipython-input-29-db56eae1d350>:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # Enable FP16\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1 - Training Loss: 0.2224\n","âœ… Training complete!\n","âœ… ResNet-50 model saved!\n","Epoch 1/1 - Training Loss: 0.3049\n","âœ… Training complete!\n","âœ… EfficientNet-B0 model saved!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:05<00:00, 92.8MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1 - Training Loss: 0.1885\n","âœ… Training complete!\n","âœ… VGG-16 model saved!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchvision.models import efficientnet_b0\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import DataLoader, Subset\n","import numpy as np\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","data_root = \"/content/VOC2008\"\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","VOC_CLASSES = [\n","    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n","    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n","]\n","class_to_idx = {cls: idx for idx, cls in enumerate(VOC_CLASSES)}\n","\n","\n","def voc_collate_fn(batch):\n","    images = []\n","    labels = []\n","\n","    for img, target in batch:\n","        images.append(img)\n","\n","        objects = target['annotation'].get('object', [])\n","        if isinstance(objects, dict):\n","            objects = [objects]\n","\n","        label_vector = torch.zeros(len(VOC_CLASSES), dtype=torch.float32)\n","        for obj in objects:\n","            if obj['name'] in class_to_idx:\n","                label_vector[class_to_idx[obj['name']]] = 1.0\n","\n","        labels.append(label_vector)\n","\n","    images = torch.stack(images, dim=0)\n","    labels = torch.stack(labels, dim=0)\n","\n","    return images, labels\n","\n","train_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"train\", transform=transform)\n","val_dataset = VOCDetection(root=data_root, year=\"2008\", image_set=\"val\", transform=transform)\n","\n","subset_size = 2000\n","train_indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n","val_indices = np.random.choice(len(val_dataset), int(subset_size * 0.2), replace=False)\n","\n","train_dataset = Subset(train_dataset, train_indices)\n","val_dataset = Subset(val_dataset, val_indices)\n","\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=voc_collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=voc_collate_fn)\n","\n","print(\"âœ… DataLoader Ready!\")\n","\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1):\n","    model = model.to(device)\n","    scaler = torch.cuda.amp.GradScaler()\n","    best_loss = float(\"inf\")\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast():\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            running_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_train_loss:.4f}\")\n","\n","    print(\"âœ… Training complete!\")\n","    return model\n","\n","\n","resnet_model = models.resnet50(pretrained=True)\n","num_ftrs = resnet_model.fc.in_features\n","resnet_model.fc = nn.Linear(num_ftrs, len(VOC_CLASSES))\n","\n","\n","for param in resnet_model.parameters():\n","    param.requires_grad = False\n","for param in resnet_model.fc.parameters():\n","    param.requires_grad = True\n","\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(resnet_model.fc.parameters(), lr=0.001)\n","\n","\n","resnet_model = train_model(resnet_model, train_loader, val_loader, criterion, optimizer, num_epochs=1)\n","torch.save(resnet_model.state_dict(), \"/content/resnet_model.pth\")\n","print(\"âœ… ResNet-50 model saved!\")\n","\n","\n","efficientnet_model = efficientnet_b0(pretrained=True)\n","num_ftrs = efficientnet_model.classifier[1].in_features\n","efficientnet_model.classifier[1] = nn.Linear(num_ftrs, len(VOC_CLASSES))\n","\n","\n","for param in efficientnet_model.parameters():\n","    param.requires_grad = False\n","for param in efficientnet_model.classifier.parameters():\n","    param.requires_grad = True\n","\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(efficientnet_model.classifier.parameters(), lr=0.001)\n","\n","efficientnet_model = train_model(efficientnet_model, train_loader, val_loader, criterion, optimizer, num_epochs=1)\n","torch.save(efficientnet_model.state_dict(), \"/content/efficientnet_model.pth\")\n","print(\"âœ… EfficientNet-B0 model saved!\")\n","\n","\n","vgg_model = models.vgg16(pretrained=True)\n","num_ftrs = vgg_model.classifier[6].in_features\n","vgg_model.classifier[6] = nn.Linear(num_ftrs, len(VOC_CLASSES))\n","\n","\n","for param in vgg_model.features.parameters():\n","    param.requires_grad = False\n","for param in vgg_model.classifier[6].parameters():\n","    param.requires_grad = True\n","\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(vgg_model.classifier[6].parameters(), lr=0.001)\n","\n","\n","vgg_model = train_model(vgg_model, train_loader, val_loader, criterion, optimizer, num_epochs=1)\n","torch.save(vgg_model.state_dict(), \"/content/vgg_model.pth\")\n","print(\"âœ… VGG-16 model saved!\")\n"]},{"cell_type":"markdown","metadata":{"id":"446mWN05u0TV"},"source":["COMPARISION B/W MODELS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJWerQcWyBj5","outputId":"547a883d-9ea9-4d82-b860-4ebcdba3596a"},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ“Š Model Comparison Results:\n","âœ… ResNet-50 Accuracy: 0.9521\n","âœ… EfficientNet-B0 Accuracy: 0.9189\n","âœ… VGG-16 Accuracy: 0.9610\n"]}],"source":["import torch\n","\n","\n","resnet_model.load_state_dict(torch.load(\"/content/resnet_model.pth\"))\n","efficientnet_model.load_state_dict(torch.load(\"/content/efficientnet_model.pth\"))\n","vgg_model.load_state_dict(torch.load(\"/content/vgg_model.pth\"))\n","\n","\n","resnet_model.eval()\n","efficientnet_model.eval()\n","vgg_model.eval()\n","\n","\n","def evaluate_model(model, val_loader):\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predicted = (torch.sigmoid(outputs) > 0.5).float()\n","            correct += (predicted == labels).sum().item()\n","            total += labels.numel()\n","\n","    return correct / total\n","\n","\n","resnet_acc = evaluate_model(resnet_model, val_loader)\n","efficientnet_acc = evaluate_model(efficientnet_model, val_loader)\n","vgg_acc = evaluate_model(vgg_model, val_loader)\n","\n","print(f\"ðŸ“Š Model Comparison Results:\")\n","print(f\"âœ… ResNet-50 Accuracy: {resnet_acc:.4f}\")\n","print(f\"âœ… EfficientNet-B0 Accuracy: {efficientnet_acc:.4f}\")\n","print(f\"âœ… VGG-16 Accuracy: {vgg_acc:.4f}\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}